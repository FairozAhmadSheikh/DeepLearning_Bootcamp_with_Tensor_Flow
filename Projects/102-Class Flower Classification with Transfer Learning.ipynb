{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **Dealing with 100+ classes, especially when some are visually similar (like different flower species), requires a more powerful technique than a simple CNN built from scratch. We'll use the Oxford Flowers 102 dataset and implement Transfer Learning using a pre-trained model like MobileNetV2.**\n",
        "\n"
      ],
      "metadata": {
        "id": "MwBYeAAd0CYZ"
      },
      "id": "MwBYeAAd0CYZ"
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "id": "bc1f1a43",
      "metadata": {
        "vscode": {
          "languageId": "plaintext"
        },
        "id": "bc1f1a43"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "import tensorflow_datasets as tfds\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#  Hyperparameters\n",
        "IMG_SIZE = 224\n",
        "BATCH_SIZE = 32\n",
        "NUM_CLASSES = 102 # The Oxford Flowers 102 dataset has 102 classes\n",
        "AUTOTUNE = tf.data.AUTOTUNE"
      ],
      "metadata": {
        "id": "-v0r7hVez6cv"
      },
      "id": "-v0r7hVez6cv",
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the Dataset\n",
        "# Oxford Flowers 102 has 'train', 'validation', and 'test' splits\n",
        "(train_ds_raw, val_ds_raw, test_ds_raw), metadata = tfds.load(\n",
        "    'oxford_flowers102',\n",
        "    split=['train', 'validation', 'test'],\n",
        "    as_supervised=True,\n",
        "    with_info=True,\n",
        ")\n",
        "\n",
        "print(f\"Number of training examples: {metadata.splits['train'].num_examples}\")\n",
        "print(f\"Number of classes: {metadata.features['label'].num_classes}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BFCMfewI0YXv",
        "outputId": "6e22b5a1-9e78-4999-a0a3-d323a7565797"
      },
      "id": "BFCMfewI0YXv",
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of training examples: 1020\n",
            "Number of classes: 102\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Preprocessing Function\n",
        "def preprocess(image, label):\n",
        "    image = tf.image.resize(image, (IMG_SIZE, IMG_SIZE))\n",
        "    image = tf.cast(image, tf.float32) / 255.0\n",
        "    return image, label"
      ],
      "metadata": {
        "id": "xmQDzv7R0x_P"
      },
      "id": "xmQDzv7R0x_P",
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Augmentation Layer\n",
        "data_augmentation = tf.keras.Sequential([\n",
        "    tf.keras.layers.RandomFlip(\"horizontal\"),\n",
        "    tf.keras.layers.RandomRotation(0.2),\n",
        "    tf.keras.layers.RandomZoom(0.1),\n",
        "    tf.keras.layers.RandomContrast(0.1),\n",
        "])"
      ],
      "metadata": {
        "id": "LsbSVGyE09xm"
      },
      "id": "LsbSVGyE09xm",
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create the Input Pipeline\n",
        "def prepare_dataset(ds, shuffle=False, augment=False):\n",
        "    ds = ds.map(preprocess, num_parallel_calls=AUTOTUNE)\n",
        "    if shuffle:\n",
        "        ds = ds.shuffle(1000)\n",
        "    ds = ds.batch(BATCH_SIZE)\n",
        "    if augment:\n",
        "        ds = ds.map(lambda x, y: (data_augmentation(x, training=True), y), num_parallel_calls=AUTOTUNE)\n",
        "    return ds.cache().prefetch(AUTOTUNE)\n"
      ],
      "metadata": {
        "id": "TSsElC-H1HGe"
      },
      "id": "TSsElC-H1HGe",
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Prepare the final datasets\n",
        "train_ds = prepare_dataset(train_ds_raw, shuffle=True, augment=True)\n",
        "val_ds = prepare_dataset(val_ds_raw, augment=False)\n",
        "test_ds = prepare_dataset(test_ds_raw, augment=False)"
      ],
      "metadata": {
        "id": "mu-ORQp91bFm"
      },
      "id": "mu-ORQp91bFm",
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Load MobileNetV2 Feature Extractor from TF Hub ---\n",
        "feature_extractor_url = \"https://tfhub.dev/google/tf2-preview/mobilenet_v2/feature_vector/4\"\n",
        "\n",
        "feature_extractor_layer = hub.KerasLayer(\n",
        "    feature_extractor_url,\n",
        "    input_shape=(IMG_SIZE, IMG_SIZE, 3),\n",
        "    trainable=False  # freeze the base model\n",
        ")\n",
        "\n",
        "# --- Build Model using Functional API ---\n",
        "inputs = tf.keras.Input(shape=(IMG_SIZE, IMG_SIZE, 3))\n",
        "x = feature_extractor_layer(inputs)\n",
        "x = tf.keras.layers.Dense(512, activation='relu')(x)\n",
        "x = tf.keras.layers.Dropout(0.3)(x)\n",
        "outputs = tf.keras.layers.Dense(NUM_CLASSES, activation='softmax')(x)\n",
        "\n",
        "model = tf.keras.Model(inputs, outputs, name=\"mobilenetv2_flowers\")\n",
        "\n",
        "# --- Compile the Model ---\n",
        "model.compile(\n",
        "    optimizer=tf.keras.optimizers.Adam(learning_rate=0.0005),\n",
        "    loss='sparse_categorical_crossentropy',\n",
        "    metrics=['accuracy']\n",
        ")\n",
        "\n",
        "# --- Model Summary ---\n",
        "model.summary()\n",
        "\n",
        "# --- Train the Model ---\n",
        "EPOCHS = 10\n",
        "history = model.fit(\n",
        "    train_ds,\n",
        "    validation_data=val_ds,\n",
        "    epochs=EPOCHS\n",
        ")\n",
        "\n",
        "# --- Evaluate on Test Dataset ---\n",
        "test_loss, test_acc = model.evaluate(test_ds)\n",
        "print(f\"\\nTest Accuracy: {test_acc:.4f}\")\n",
        "print(f\"Test Loss: {test_loss:.4f}\")\n",
        "\n",
        "# --- Fine-tuning (optional) ---\n",
        "feature_extractor_layer.trainable = True\n",
        "model.compile(\n",
        "    optimizer=tf.keras.optimizers.Adam(learning_rate=1e-5),\n",
        "    loss='sparse_categorical_crossentropy',\n",
        "    metrics=['accuracy']\n",
        ")\n",
        "\n",
        "fine_tune_epochs = 5\n",
        "total_epochs = EPOCHS + fine_tune_epochs\n",
        "\n",
        "history_fine = model.fit(\n",
        "    train_ds,\n",
        "    validation_data=val_ds,\n",
        "    epochs=total_epochs,\n",
        "    initial_epoch=history.epoch[-1]\n",
        ")\n",
        "\n",
        "# --- Evaluate after Fine-Tuning ---\n",
        "test_loss, test_acc = model.evaluate(test_ds)\n",
        "print(f\"\\nFine-tuned Test Accuracy: {test_acc:.4f}\")\n",
        "\n",
        "# --- Plot Training History ---\n",
        "acc = history.history['accuracy'] + history_fine.history['accuracy']\n",
        "val_acc = history.history['val_accuracy'] + history_fine.history['val_accuracy']\n",
        "loss = history.history['loss'] + history_fine.history['loss']\n",
        "val_loss = history.history['val_loss'] + history_fine.history['val_loss']\n",
        "\n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(acc, label='Training Accuracy')\n",
        "plt.plot(val_acc, label='Validation Accuracy')\n",
        "plt.title('Accuracy')\n",
        "plt.legend()\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(loss, label='Training Loss')\n",
        "plt.plot(val_loss, label='Validation Loss')\n",
        "plt.title('Loss')\n",
        "plt.legend()\n",
        "\n",
        "plt.show()\n",
        "\n",
        "# --- Make Predictions on Test Samples ---\n",
        "class_names = metadata.features['label'].names\n",
        "\n",
        "for images, labels in test_ds.take(1):\n",
        "    preds = model.predict(images)\n",
        "    pred_labels = tf.argmax(preds, axis=1)\n",
        "\n",
        "    plt.figure(figsize=(12, 12))\n",
        "    for i in range(9):\n",
        "        ax = plt.subplot(3, 3, i + 1)\n",
        "        plt.imshow(images[i])\n",
        "        true_label = class_names[labels[i].numpy()]\n",
        "        pred_label = class_names[pred_labels[i].numpy()]\n",
        "        color = \"green\" if true_label == pred_label else \"red\"\n",
        "        plt.title(f\"True: {true_label}\\nPred: {pred_label}\", color=color)\n",
        "        plt.axis(\"off\")\n",
        "    plt.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 512
        },
        "id": "f-pVN7Fr1jdv",
        "outputId": "c07afcaf-87e1-4812-ddc5-888ff01f6401"
      },
      "id": "f-pVN7Fr1jdv",
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "Exception encountered when calling layer 'keras_layer_7' (type KerasLayer).\n\nA KerasTensor is symbolic: it's a placeholder for a shape an a dtype. It doesn't have any actual numerical value. You cannot convert it to a NumPy array.\n\nCall arguments received by layer 'keras_layer_7' (type KerasLayer):\n  • inputs=<KerasTensor shape=(None, 224, 224, 3), dtype=float32, sparse=False, ragged=False, name=keras_tensor_9>\n  • training=None",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-1831040501.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;31m# --- Build Model using Functional API ---\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0minputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mInput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mIMG_SIZE\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mIMG_SIZE\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfeature_extractor_layer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDense\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m512\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactivation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'relu'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDropout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0.3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/tf_keras/src/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     68\u001b[0m             \u001b[0;31m# To get the full stack trace, call:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m             \u001b[0;31m# `tf.debugging.disable_traceback_filtering()`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 70\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     71\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m             \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/tensorflow_hub/keras_layer.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, inputs, training)\u001b[0m\n\u001b[1;32m    248\u001b[0m         \u001b[0;31m# Behave like BatchNormalization. (Dropout is different, b/181839368.)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    249\u001b[0m         \u001b[0mtraining\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 250\u001b[0;31m       result = smart_cond.smart_cond(training,\n\u001b[0m\u001b[1;32m    251\u001b[0m                                      \u001b[0;32mlambda\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtraining\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    252\u001b[0m                                      lambda: f(training=False))\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/tensorflow_hub/keras_layer.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    250\u001b[0m       result = smart_cond.smart_cond(training,\n\u001b[1;32m    251\u001b[0m                                      \u001b[0;32mlambda\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtraining\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 252\u001b[0;31m                                      lambda: f(training=False))\n\u001b[0m\u001b[1;32m    253\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    254\u001b[0m     \u001b[0;31m# Unwrap dicts returned by signatures.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/tensorflow/core/function/polymorphism/function_type.py\u001b[0m in \u001b[0;36mcanonicalize_to_monomorphic\u001b[0;34m(args, kwargs, default_values, capture_types, polymorphic_type)\u001b[0m\n\u001b[1;32m    585\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    586\u001b[0m       parameters.append(\n\u001b[0;32m--> 587\u001b[0;31m           _make_validated_mono_param(name, arg, poly_parameter.kind,\n\u001b[0m\u001b[1;32m    588\u001b[0m                                      \u001b[0mtype_context\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    589\u001b[0m                                      poly_parameter.type_constraint))\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/tensorflow/core/function/polymorphism/function_type.py\u001b[0m in \u001b[0;36m_make_validated_mono_param\u001b[0;34m(name, value, kind, type_context, poly_type)\u001b[0m\n\u001b[1;32m    524\u001b[0m ) -> Parameter:\n\u001b[1;32m    525\u001b[0m   \u001b[0;34m\"\"\"Generates and validates a parameter for Monomorphic FunctionType.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 526\u001b[0;31m   \u001b[0mmono_type\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrace_type\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_value\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtype_context\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    527\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    528\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mpoly_type\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mmono_type\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_subtype_of\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpoly_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/tensorflow/core/function/trace_type/trace_type_builder.py\u001b[0m in \u001b[0;36mfrom_value\u001b[0;34m(value, context)\u001b[0m\n\u001b[1;32m    183\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    184\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mutil\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_np_ndarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 185\u001b[0;31m     \u001b[0mndarray\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__array__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    186\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdefault_types\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTENSOR\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mndarray\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mndarray\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    187\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/keras/src/backend/common/keras_tensor.py\u001b[0m in \u001b[0;36m__array__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    124\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    125\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__array__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 126\u001b[0;31m         raise ValueError(\n\u001b[0m\u001b[1;32m    127\u001b[0m             \u001b[0;34m\"A KerasTensor is symbolic: it's a placeholder for a shape \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    128\u001b[0m             \u001b[0;34m\"an a dtype. It doesn't have any actual numerical value. \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Exception encountered when calling layer 'keras_layer_7' (type KerasLayer).\n\nA KerasTensor is symbolic: it's a placeholder for a shape an a dtype. It doesn't have any actual numerical value. You cannot convert it to a NumPy array.\n\nCall arguments received by layer 'keras_layer_7' (type KerasLayer):\n  • inputs=<KerasTensor shape=(None, 224, 224, 3), dtype=float32, sparse=False, ragged=False, name=keras_tensor_9>\n  • training=None"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU",
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}